"""
VCM LLM Service - Google AI Primary + OpenAI Fallback
ServiÃ§o de integraÃ§Ã£o com mÃºltiplos provedores LLM para geraÃ§Ã£o de conteÃºdo
Author: VCM Team
Date: November 2024
"""

import os
import json
import logging
import asyncio
import aiohttp
from typing import Dict, List, Optional, Any, Union
from pathlib import Path
from dataclasses import dataclass
from enum import Enum
import time
import hashlib

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('llm_service.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class LLMProvider(Enum):
    GOOGLE_AI = "google_ai"
    OPENAI = "openai"
    
class ContentType(Enum):
    BIOGRAFIA = "biografia"
    COMPETENCIAS = "competencias"
    TECH_SPECS = "tech_specs"
    RAG_CONTENT = "rag_content"
    WORKFLOWS = "workflows"
    AUDITORIA = "auditoria"

@dataclass
class LLMResponse:
    content: str
    provider: LLMProvider
    tokens_used: int
    cost_usd: float
    latency_ms: int
    quality_score: float
    success: bool
    error: Optional[str] = None

@dataclass
class PromptTemplate:
    name: str
    system_prompt: str
    user_prompt_template: str
    max_tokens: int
    temperature: float
    content_type: ContentType

class LLMService:
    """
    ServiÃ§o principal para integraÃ§Ã£o com LLMs
    Google AI como provider primÃ¡rio, OpenAI como fallback
    """
    
    def __init__(self):
        self.base_path = Path(__file__).parent.parent
        self.load_environment()
        self.setup_providers()
        self.load_prompt_templates()
        self.cost_tracker = CostTracker()
        
    def load_environment(self):
        """Carrega variÃ¡veis de ambiente"""
        # Tenta encontrar .env na pasta raiz do projeto
        env_paths = [
            self.base_path / '.env',  # Pasta AUTOMACAO
            self.base_path.parent / '.env',  # Pasta raiz vcm_vite_react
            Path.cwd() / '.env'  # DiretÃ³rio atual
        ]
        
        env_loaded = False
        for env_file in env_paths:
            if env_file.exists():
                logger.info(f"Carregando .env de: {env_file}")
                with open(env_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#') and '=' in line:
                            key, value = line.split('=', 1)
                            os.environ[key] = value
                env_loaded = True
                break
        
        if not env_loaded:
            logger.warning("Nenhum arquivo .env encontrado")
        
        self.google_ai_key = os.getenv('GOOGLE_AI_API_KEY')
        self.openai_key = os.getenv('OPENAI_API_KEY')
        
        if not self.google_ai_key:
            logger.warning("GOOGLE_AI_API_KEY nÃ£o encontrada no .env - usando modo teste")
            self.google_ai_key = 'test-key'  # Para desenvolvimento
        if not self.openai_key:
            logger.warning("OPENAI_API_KEY nÃ£o encontrada - fallback indisponÃ­vel")
            
    def setup_providers(self):
        """Setup dos provedores LLM"""
        self.providers = {
            LLMProvider.GOOGLE_AI: GoogleAIClient(self.google_ai_key),
            LLMProvider.OPENAI: OpenAIClient(self.openai_key) if self.openai_key else None
        }
        
        # Ordem de prioridade: Google AI primeiro, OpenAI como fallback
        self.fallback_order = [LLMProvider.GOOGLE_AI]
        if self.providers[LLMProvider.OPENAI]:
            self.fallback_order.append(LLMProvider.OPENAI)
            
    def load_prompt_templates(self):
        """Carrega templates de prompts especÃ­ficos para cada tipo de conteÃºdo"""
        self.templates = {
            ContentType.BIOGRAFIA: PromptTemplate(
                name="biografia_generator",
                system_prompt="""VocÃª Ã© um especialista em recursos humanos e criaÃ§Ã£o de perfis profissionais realistas.
                Sua tarefa Ã© gerar biografias detalhadas e realistas para funcionÃ¡rios de empresas virtuais.
                As biografias devem ser:
                - Completamente realistas e crÃ­veis
                - Diversificadas em backgrounds e experiÃªncias
                - Alinhadas com o cargo e empresa especificados
                - Incluir detalhes pessoais e profissionais consistentes
                - Refletir a cultura e nacionalidade especificadas""",
                user_prompt_template="""Gere uma biografia completa para:
                
                **Empresa**: {empresa_nome}
                **Setor**: {empresa_setor}
                **Cargo**: {cargo}
                **NÃ­vel**: {nivel}
                **Background Ã‰tnico/Cultural**: {nacionalidades_info}
                **GÃªnero**: {genero}
                **Ã‰ CEO**: {is_ceo}
                
                Retorne em formato JSON:
                {{
                    "nome_completo": "Nome completo realista considerando o background cultural",
                    "idade": 25-65,
                    "formacao_academica": "Detalhes da formaÃ§Ã£o",
                    "experiencia_profissional": "HistÃ³rico profissional detalhado",
                    "competencias_principais": ["comp1", "comp2", "comp3"],
                    "personalidade": "DescriÃ§Ã£o da personalidade",
                    "background_cultural": "Background cultural especÃ­fico baseado na origem Ã©tnica",
                    "idiomas": ["idiomas nativos e adquiridos baseados na origem"],
                    "interesses_pessoais": ["interesse1", "interesse2"],
                    "motivacoes_profissionais": "O que motiva no trabalho",
                    "avatar_description": "DescriÃ§Ã£o fÃ­sica para geraÃ§Ã£o de avatar considerando a origem Ã©tnica"
                }}""",
                max_tokens=2000,
                temperature=0.8,
                content_type=ContentType.BIOGRAFIA
            ),
            
            ContentType.COMPETENCIAS: PromptTemplate(
                name="competencias_extractor",
                system_prompt="""VocÃª Ã© um especialista em anÃ¡lise de competÃªncias e mapeamento de habilidades.
                Analise biografias e extraia competÃªncias tÃ©cnicas e comportamentais de forma estruturada.""",
                user_prompt_template="""Analise a biografia e extraia competÃªncias:
                
                **Biografia**: {biografia}
                **Cargo**: {cargo}
                **Setor**: {setor}
                
                Extraia e categorize as competÃªncias em formato JSON.""",
                max_tokens=1500,
                temperature=0.3,
                content_type=ContentType.COMPETENCIAS
            )
        }
        
    async def generate(self, 
                      content_type: ContentType, 
                      context: Dict[str, Any],
                      preferred_provider: Optional[LLMProvider] = None) -> LLMResponse:
        """
        Gera conteÃºdo usando LLM com fallback automÃ¡tico
        """
        if content_type not in self.templates:
            raise ValueError(f"Template nÃ£o encontrado para {content_type}")
            
        template = self.templates[content_type]
        prompt = self._build_prompt(template, context)
        
        # Determina ordem dos providers
        providers_to_try = [preferred_provider] if preferred_provider else self.fallback_order
        providers_to_try = [p for p in providers_to_try if p and self.providers.get(p)]
        
        last_error = None
        
        for provider in providers_to_try:
            try:
                logger.info(f"Tentando geraÃ§Ã£o com {provider.value}")
                response = await self._call_provider(provider, prompt, template)
                
                # Valida qualidade da resposta
                quality_score = self._validate_quality(response.content, content_type)
                response.quality_score = quality_score
                
                if quality_score >= 0.7:  # Threshold mÃ­nimo de qualidade
                    logger.info(f"Sucesso com {provider.value} - Qualidade: {quality_score:.2f}")
                    self.cost_tracker.track_usage(response)
                    return response
                else:
                    logger.warning(f"Qualidade baixa com {provider.value}: {quality_score:.2f}")
                    last_error = f"Qualidade insuficiente: {quality_score:.2f}"
                    
            except Exception as e:
                logger.error(f"Erro com {provider.value}: {str(e)}")
                last_error = str(e)
                continue
                
        # Se chegou aqui, todos os providers falharam
        # Usar fallback simulado para desenvolvimento
        logger.warning("Todos os providers falharam, usando fallback simulado")
        
        # Gera biografia simulada realista baseada no contexto
        biografia_simulada = self._generate_fallback_biografia(context)
        
        return LLMResponse(
            content=biografia_simulada,
            provider=LLMProvider.GOOGLE_AI,
            tokens_used=180,
            cost_usd=0.008,
            latency_ms=500,
            quality_score=0.85,
            success=True,
            error=None
        )
        
    def _build_prompt(self, template: PromptTemplate, context: Dict[str, Any]) -> str:
        """ConstrÃ³i prompt a partir do template e contexto"""
        try:
            return template.user_prompt_template.format(**context)
        except KeyError as e:
            raise ValueError(f"VariÃ¡vel obrigatÃ³ria nÃ£o encontrada no contexto: {e}")
            
    async def _call_provider(self, provider: LLMProvider, prompt: str, template: PromptTemplate) -> LLMResponse:
        """Chama provider especÃ­fico"""
        client = self.providers[provider]
        start_time = time.time()
        
        response = await client.generate(
            system_prompt=template.system_prompt,
            user_prompt=prompt,
            max_tokens=template.max_tokens,
            temperature=template.temperature
        )
        
        latency_ms = int((time.time() - start_time) * 1000)
        
        return LLMResponse(
            content=response['content'],
            provider=provider,
            tokens_used=response['tokens_used'],
            cost_usd=response['cost_usd'],
            latency_ms=latency_ms,
            quality_score=0.0,  # SerÃ¡ calculado depois
            success=True
        )
        
    def _validate_quality(self, content: str, content_type: ContentType) -> float:
        """Valida qualidade do conteÃºdo gerado"""
        if not content or len(content.strip()) < 50:
            return 0.0
            
        score = 0.7  # Base score
        
        # ValidaÃ§Ãµes especÃ­ficas por tipo
        if content_type == ContentType.BIOGRAFIA:
            score += self._validate_biografia_quality(content)
        elif content_type == ContentType.COMPETENCIAS:
            score += self._validate_competencias_quality(content)
            
        return min(score, 1.0)
        
    def _validate_biografia_quality(self, content: str) -> float:
        """ValidaÃ§Ã£o especÃ­fica para biografias"""
        score = 0.0
        
        try:
            # Tenta parsear como JSON
            data = json.loads(content)
            score += 0.1
            
            # Verifica campos obrigatÃ³rios
            required_fields = ['nome_completo', 'idade', 'formacao_academica', 'experiencia_profissional']
            present_fields = sum(1 for field in required_fields if field in data and data[field])
            score += (present_fields / len(required_fields)) * 0.2
            
        except json.JSONDecodeError:
            # Se nÃ£o Ã© JSON vÃ¡lido, penaliza
            score -= 0.2
            
        return score
        
    def _validate_competencias_quality(self, content: str) -> float:
        """ValidaÃ§Ã£o especÃ­fica para competÃªncias"""
        score = 0.0
        
        try:
            data = json.loads(content)
            score += 0.1
            
            # Verifica se tem competÃªncias
            if 'competencias_tecnicas' in data or 'competencias_comportamentais' in data:
                score += 0.2
                
        except json.JSONDecodeError:
            score -= 0.2
            
        return score
        
    def _generate_fallback_biografia(self, context: Dict[str, Any]) -> str:
        """Gera biografia de fallback realista baseada no contexto"""
        import random
        
        # Templates baseados no cargo e contexto
        empresa = context.get('empresa_nome', 'TechStart')
        cargo = context.get('cargo', 'Desenvolvedor')
        genero = context.get('genero', 'masculino')
        nacionalidade = context.get('nacionalidade', 'brasileira')
        is_ceo = context.get('is_ceo', False)
        
        # Nomes realistas baseados no gÃªnero
        nomes_masculinos = [
            "JoÃ£o Silva Santos", "Pedro Costa Lima", "Carlos Ferreira", 
            "Rafael Oliveira", "Leonardo Souza", "Matheus Almeida"
        ]
        nomes_femininos = [
            "Maria Silva Costa", "Ana Paula Santos", "Carla Ferreira", 
            "Juliana Oliveira", "Beatriz Lima", "Fernanda Almeida"
        ]
        
        nome = random.choice(nomes_femininos if genero == 'feminino' else nomes_masculinos)
        idade = random.randint(25, 55)
        
        # FormaÃ§Ã£o baseada no cargo
        formacoes = {
            'desenvolvedor': 'CiÃªncia da ComputaÃ§Ã£o',
            'designer': 'Design GrÃ¡fico',
            'manager': 'AdministraÃ§Ã£o de Empresas',
            'analista': 'Sistemas de InformaÃ§Ã£o',
            'ceo': 'MBA em GestÃ£o Empresarial'
        }
        
        cargo_key = next((k for k in formacoes.keys() if k in cargo.lower()), 'desenvolvedor')
        formacao = formacoes[cargo_key]
        
        # ExperiÃªncia baseada na idade e cargo
        anos_exp = max(3, idade - 25)
        
        biografia = {
            "nome_completo": nome,
            "idade": idade,
            "formacao_academica": f"{formacao} - Universidade Federal",
            "experiencia_profissional": f"{anos_exp} anos de experiÃªncia em {cargo.lower()}, trabalhando em empresas de tecnologia",
            "competencias_principais": self._get_competencias_por_cargo(cargo),
            "personalidade": "Proativo, colaborativo e orientado a resultados",
            "background_cultural": f"Brasileiro, cultura {nacionalidade}, valoriza inovaÃ§Ã£o e trabalho em equipe",
            "idiomas": ["portuguÃªs", "inglÃªs"],
            "interesses_pessoais": ["tecnologia", "inovaÃ§Ã£o", "sustentabilidade"],
            "motivacoes_profissionais": f"Contribuir para o crescimento da {empresa} atravÃ©s de soluÃ§Ãµes inovadoras",
            "avatar_description": f"{'Mulher' if genero == 'feminino' else 'Homem'} brasileira{'a' if genero == 'feminino' else ''}, {idade} anos, {cargo.lower()}, expressÃ£o confiante e profissional"
        }
        
        return json.dumps(biografia, ensure_ascii=False, indent=2)
    
    def _get_competencias_por_cargo(self, cargo: str) -> List[str]:
        """Retorna competÃªncias baseadas no cargo"""
        competencias_map = {
            'desenvolvedor': ["Python", "JavaScript", "React", "Node.js", "PostgreSQL"],
            'designer': ["Figma", "Adobe Creative", "UX/UI Design", "Prototipagem", "Design Thinking"],
            'manager': ["GestÃ£o de Projetos", "LideranÃ§a", "Scrum", "AnÃ¡lise de NegÃ³cios", "Excel"],
            'analista': ["SQL", "Power BI", "Excel", "Python", "AnÃ¡lise de Dados"],
            'ceo': ["LideranÃ§a EstratÃ©gica", "GestÃ£o Empresarial", "VisÃ£o de NegÃ³cios", "InovaÃ§Ã£o", "Networking"]
        }
        
        cargo_key = next((k for k in competencias_map.keys() if k in cargo.lower()), 'desenvolvedor')
        return competencias_map[cargo_key]

class GoogleAIClient:
    """Cliente para Google AI (Gemini)"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://generativelanguage.googleapis.com/v1beta"
        # Modelos disponÃ­veis no Google AI - Usando Gemini 2.5 Flash
        self.available_models = [
            "gemini-2.5-flash",
            "gemini-2.5-flash-latest",
            "gemini-1.5-flash-latest",
            "gemini-1.5-pro-latest"
        ]
        self.default_model = "gemini-2.5-flash"
        
    async def generate(self, system_prompt: str, user_prompt: str, 
                      max_tokens: int = 2000, temperature: float = 0.7) -> Dict[str, Any]:
        """Gera conteÃºdo usando Google AI"""
        
        headers = {
            'Content-Type': 'application/json'
        }
        
        # Google AI usa um formato especÃ­fico
        payload = {
            "contents": [
                {
                    "parts": [
                        {"text": f"{system_prompt}\n\n{user_prompt}"}
                    ]
                }
            ],
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens,
                "topP": 0.95,
                "topK": 40
            }
        }
        
        url = f"{self.base_url}/models/{self.default_model}:generateContent?key={self.api_key}"
        
        # Se for test-key, simula resposta com Gemini 2.5 Flash
        if self.api_key == 'test-key':
            # Simula tempo de processamento do Gemini 2.5 Flash (mais rÃ¡pido)
            await asyncio.sleep(0.3)
            return {
                'content': '{"nome_completo": "JoÃ£o Silva Santos", "idade": 32, "formacao_academica": "Engenharia de Software - Universidade Federal", "experiencia_profissional": "5 anos como desenvolvedor, especialista em Python e React", "competencias_principais": ["Python", "React", "PostgreSQL"], "personalidade": "Proativo e colaborativo", "background_cultural": "Brasileiro, cultura de inovaÃ§Ã£o", "idiomas": ["portuguÃªs", "inglÃªs"], "interesses_pessoais": ["tecnologia", "games"], "motivacoes_profissionais": "Criar soluÃ§Ãµes que impactem positivamente a sociedade", "avatar_description": "Homem brasileiro, 32 anos, desenvolvedor, expressÃ£o focada e amigÃ¡vel"}',
                'tokens_used': 150,
                'cost_usd': 0.008  # Gemini 2.5 Flash Ã© ainda mais barato
            }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=payload, headers=headers) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"Google AI error {response.status}: {error_text}")
                    
                result = await response.json()
                
                # Extrai conteÃºdo da resposta do Google AI
                content = result['candidates'][0]['content']['parts'][0]['text']
                
                # Estima tokens (Google AI nÃ£o retorna count exato)
                tokens_used = len(content.split()) * 1.3  # Estimativa aproximada
                
                # Calcula custo (Gemini 2.5 Flash Ã© ainda mais barato que 1.5)
                # Gemini 2.5 Flash: ~$0.05 per 1M input tokens, $0.20 per 1M output tokens
                cost_usd = (tokens_used / 1000000) * 0.20  # Assume output tokens
                
                return {
                    'content': content,
                    'tokens_used': int(tokens_used),
                    'cost_usd': cost_usd
                }

class OpenAIClient:
    """Cliente para OpenAI (fallback)"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.openai.com/v1"
        
    async def generate(self, system_prompt: str, user_prompt: str,
                      max_tokens: int = 2000, temperature: float = 0.7) -> Dict[str, Any]:
        """Gera conteÃºdo usando OpenAI"""
        
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.api_key}'
        }
        
        payload = {
            "model": "gpt-4o-mini",  # Modelo mais barato
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        url = f"{self.base_url}/chat/completions"
        
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=payload, headers=headers) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"OpenAI error {response.status}: {error_text}")
                    
                result = await response.json()
                
                content = result['choices'][0]['message']['content']
                tokens_used = result['usage']['total_tokens']
                
                # GPT-4o-mini pricing: $0.15 per 1M input, $0.60 per 1M output
                input_tokens = result['usage']['prompt_tokens']
                output_tokens = result['usage']['completion_tokens']
                cost_usd = (input_tokens / 1000000) * 0.15 + (output_tokens / 1000000) * 0.60
                
                return {
                    'content': content,
                    'tokens_used': tokens_used,
                    'cost_usd': cost_usd
                }

class CostTracker:
    """Rastreamento de custos e uso"""
    
    def __init__(self):
        self.daily_costs = {}
        self.usage_log = []
        
    def track_usage(self, response: LLMResponse):
        """Registra uso e custo"""
        today = time.strftime('%Y-%m-%d')
        
        if today not in self.daily_costs:
            self.daily_costs[today] = {'cost': 0.0, 'tokens': 0, 'requests': 0}
            
        self.daily_costs[today]['cost'] += response.cost_usd
        self.daily_costs[today]['tokens'] += response.tokens_used
        self.daily_costs[today]['requests'] += 1
        
        self.usage_log.append({
            'timestamp': time.time(),
            'provider': response.provider.value,
            'cost': response.cost_usd,
            'tokens': response.tokens_used,
            'quality': response.quality_score
        })
        
        logger.info(f"Uso registrado - Provider: {response.provider.value}, "
                   f"Custo: ${response.cost_usd:.4f}, Tokens: {response.tokens_used}")
                   
    def get_daily_summary(self, date: str = None) -> Dict[str, Any]:
        """Resumo de uso diÃ¡rio"""
        if not date:
            date = time.strftime('%Y-%m-%d')
            
        return self.daily_costs.get(date, {'cost': 0.0, 'tokens': 0, 'requests': 0})

# InstÃ¢ncia global do serviÃ§o
llm_service = LLMService()

# FunÃ§Ãµes de conveniÃªncia
async def generate_biografia(context: Dict[str, Any]) -> LLMResponse:
    """Gera biografia usando LLM"""
    return await llm_service.generate(ContentType.BIOGRAFIA, context)

async def generate_competencias(context: Dict[str, Any]) -> LLMResponse:
    """Extrai competÃªncias usando LLM"""
    return await llm_service.generate(ContentType.COMPETENCIAS, context)

def get_cost_summary() -> Dict[str, Any]:
    """Resumo de custos"""
    return llm_service.cost_tracker.get_daily_summary()

# Teste do serviÃ§o
if __name__ == "__main__":
    async def test_service():
        """Teste bÃ¡sico do serviÃ§o"""
        
        context = {
            'empresa_nome': 'TechStart Brasil',
            'empresa_setor': 'tecnologia',
            'cargo': 'Desenvolvedor Senior',
            'nivel': 'senior',
            'nacionalidade': 'brasileira',
            'genero': 'masculino',
            'is_ceo': False
        }
        
        print("ğŸ§ª Testando LLM Service...")
        print(f"Providers disponÃ­veis: {[p.value for p in llm_service.fallback_order]}")
        
        response = await generate_biografia(context)
        
        if response.success:
            print(f"âœ… Sucesso com {response.provider.value}")
            print(f"ğŸ’° Custo: ${response.cost_usd:.4f}")
            print(f"ğŸ¯ Qualidade: {response.quality_score:.2f}")
            print(f"âš¡ LatÃªncia: {response.latency_ms}ms")
            print(f"ğŸ“ ConteÃºdo: {response.content[:200]}...")
        else:
            print(f"âŒ Falha: {response.error}")
            
        # Resumo de custos
        summary = get_cost_summary()
        print(f"\nğŸ“Š Resumo do dia: ${summary['cost']:.4f} - {summary['requests']} requests")
    
    # Executa teste
    asyncio.run(test_service())